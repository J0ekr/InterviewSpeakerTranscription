{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9246b310-5eaa-4b55-b345-44da582da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these if not there\n",
    "# !pip install pydub\n",
    "# !pip install light-the-torch\n",
    "# !pip install  git+https://github.com/hmmlearn/hmmlearn.git\n",
    "# !pip install  git+https://github.com/pyannote/pyannote-audio.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c865be21-9ed4-48b1-b398-00a2c63df253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 10:15:43.703412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-02 10:15:43.703466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-02 10:15:43.704997: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-02 10:15:43.716856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 10:15:44.530593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Enter either 0 or 1 below to select the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543405c4-2568-4783-b64c-5d6ff91915ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/opt/jupyterhub/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyannote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.11/site-packages/pyannote/audio/__init__.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Inference\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.11/site-packages/pyannote/audio/core/inference.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Text, Tuple, Union\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.11/site-packages/torch/__init__.py:239\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    238\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /opt/jupyterhub/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: ncclCommRegister"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import speechbrain as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfad9c3-ef4e-459a-9c4b-483d22e7c8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a3ad0-e565-4605-9938-4c52d44b333b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034b98c-7eac-432a-918f-eb80c2042644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac5fe2-835d-4447-b15b-6acfa801b7d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### With first usage of whisper first needs to install/update \"numba.jit\" - happens automatically - 1.42gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798b481-e4f6-4f08-bbdb-a007cad43f86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate a token from hugginface and insert it below\n",
    "1. visit hf.co/pyannote/speaker-diarization and accept user conditions\n",
    "2. visit hf.co/pyannote/segmentation and accept user conditions\n",
    "3. visit hf.co/settings/tokens to create an access token\n",
    "4. instantiate pretrained speaker diarization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558beeed-94f5-41bc-a080-a3b0d0362a95",
   "metadata": {},
   "source": [
    "# Things to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de37dd2-e202-4028-94b0-0ccc99ba2412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_token = \"<HF_TOKEN>\" # You need to create this and enter here as explained above\n",
    "NUM_SPEAKERS = 2 # Amount of people who are speaking.\n",
    "LANGUAGE = \"en\" # English: 'en' German: 'de'\n",
    "data = \"./dataFolder\" # Folder with all your m4a audio files nright next to this file\n",
    "dataOut = \"./outFolder\" # Create this folder for all the output to be saved in\n",
    "types = ('**/*.m4a', '**/*.mp3', '**/*.ogg', '**/*.aac') # wav does not work right now due to how I implemented things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27412d07-05a1-41e5-9727-b14f1ea87c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# convert m4a to wav and create subfolder structure and add spacer before file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5699d-02c2-4dc2-8442-812b1ac05b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function which does all steps for one file \n",
    "spacermilli = 2000\n",
    "def convertAudiofiles(audio_file):\n",
    "    f = f\"{str(audio_file).split('.')[0]}\"\n",
    "    print(f.split(\"/\")[1])\n",
    "    f =  f.replace('data', 'out')\n",
    "    # Create folders\n",
    "    !mkdir {f} \n",
    "    ftmp = f\"{f}/tmp\"\n",
    "    !mkdir {ftmp}\n",
    "    fa = f\"{f}/{str(audio_file).split('/')[1].split('.')[0]}.wav\"\n",
    "    print(f, fa)\n",
    "    tmp = {\"name\": f.split(\"/\")[1], \"og_audio\": fa, \"tmp_folder\": ftmp}\n",
    "    !ffmpeg -y -i {audio_file} {fa}\n",
    "    \n",
    "    # tmp[\"proc_audio\"] = tmp[\"og_audio\"].split(\".\")[0] +  \"_prep.wav\"\n",
    "    # spacer = AudioSegment.silent(duration=spacermilli)\n",
    "    # audio = AudioSegment.from_wav(tmp[\"og_audio\"]) \n",
    "    # audio.export(tmp[\"og_audio\"], format='wav')\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92800905-4267-4e00-8147-08a64619915d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_files = []\n",
    "for fileending in types:\n",
    "    pathlist = Path(data).glob(fileending)\n",
    "    for audio_file in pathlist:\n",
    "        audio_files.append(convertAudiofiles(audio_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a4cf4-6b8a-4f69-b716-c48ec5ae375f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Append time before actual audio, else pyannot might not pick up the first 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50dd1d4-a913-4b77-ae54-8bda4a783ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    processed_audio_file = file[\"og_audio\"]\n",
    "    # processed_audio_file2 = file[\"og_audio\"].split(\".\")[0] +  \"_prep.wav\"\n",
    "    # file[\"proc_audio\"] = processed_audio_file2\n",
    "    spacer = AudioSegment.silent(duration=spacermilli)\n",
    "    audio = AudioSegment.from_wav(processed_audio_file) \n",
    "    audio = spacer.append(audio, crossfade=0)\n",
    "    audio.export(file[\"og_audio\"], format='wav')\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b72ca-1248-4546-a44a-dc64645ee1a7",
   "metadata": {},
   "source": [
    "# Init Pyannot pipeline for speaker identification and whisper for transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840063dd-a6ec-4040-a0b4-5da83202820a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=hf_token)\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "device = torch.device(\"cuda\")\n",
    "model = whisper.load_model('large', device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb70460-c79b-4e72-8bcd-e518a7d1ae9b",
   "metadata": {},
   "source": [
    "# Create diarization file (when does a new speaker speak) - this will take a long while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef299360-a083-4a55-a10e-4a7bb6111108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diarization_file(audiofile, numspeaker):\n",
    "    if numspeaker == None:\n",
    "        dz = pipeline(audiofile)  \n",
    "    else:\n",
    "        dz = pipeline(audiofile, num_speakers=numspeaker) \n",
    "    diarization_file = audiofile.split(\".\")[0] + \"_dia.txt\"\n",
    "    print(diarization_file)\n",
    "    if os.path.isfile(diarization_file):\n",
    "        return diarization_file\n",
    "    with open(diarization_file, \"w\") as text_file:\n",
    "        text_file.write(str(dz))\n",
    "    return diarization_file\n",
    "\n",
    "def millisec(timeStr):\n",
    "    spl = timeStr.split(\":\")\n",
    "    s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4d49d-9052-4a9c-b8fe-d9f2890c6a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    file[\"dia_file\"] = create_diarization_file(file[\"og_audio\"], NUM_SPEAKERS)\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba0b9e-f6de-47af-95a9-e791237f2387",
   "metadata": {},
   "source": [
    "# Create audio segments based on the previous timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41aa0bc-535a-4d19-83ea-a5fe1cd038f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833b953-2f77-4c01-84db-b14c31834910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(file):\n",
    "    dzs = open(file[\"dia_file\"]).read().splitlines()\n",
    "    groups = []\n",
    "    g = []\n",
    "    lastend = 0\n",
    "    for d in dzs:   \n",
    "        if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
    "            groups.append(g)\n",
    "            g = []\n",
    "\n",
    "        g.append(d)\n",
    "\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
    "        end = millisec(end)\n",
    "        if (lastend > end):       #segment engulfed by a previous segment\n",
    "            groups.append(g)\n",
    "            g = [] \n",
    "        else:\n",
    "            lastend = end\n",
    "    if g:\n",
    "        groups.append(g)\n",
    "    audio = AudioSegment.from_wav(file[\"og_audio\"])\n",
    "    \n",
    "    gidx = -1\n",
    "    tmp_files = []\n",
    "    for g in groups:\n",
    "        start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "        start = millisec(start) #- spacermilli\n",
    "        end = millisec(end)  #- spacermilli\n",
    "        gidx += 1\n",
    "        fname = f\"{file['tmp_folder']}/{gidx}.wav\"\n",
    "        tmp_files.append(fname)\n",
    "        audio[start:end].export(fname, format='wav')\n",
    "    print(tmp_files[:10])\n",
    "    return tmp_files, groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3505d-de58-4b2c-beea-a1ba5325f1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    file[\"tmp_wavs\"], file[\"groups\"] = create_segments(file)\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b299c-b201-47bb-87dd-4b97d622556f",
   "metadata": {},
   "source": [
    "# Create Transcripts for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe677c9-4616-4e3b-ad4c-ecf085b95760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_segment_transcripts(file, language='en'):\n",
    "    # print(len(file[\"tmp_wavs\"]))\n",
    "    for i in range(len(file[\"tmp_wavs\"])):\n",
    "        audiof = str(i) + '.wav'\n",
    "        segment_audio = f\"{file['tmp_folder']}/{audiof}\"\n",
    "        result = model.transcribe(audio=segment_audio, language=language, word_timestamps=True)#, initial_prompt=result.get('text', \"\"))\n",
    "        o = f\"{file['tmp_folder']}/\"\n",
    "        tmp_files = []\n",
    "        # print(f\"{o}{i}.json\")\n",
    "        with open(f\"{o}{i}.json\", \"w\") as outfile:\n",
    "            tmp_files.append(str(outfile))\n",
    "            json.dump(result, outfile, indent=4) \n",
    "    return tmp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce114eba-6681-448d-b9ad-91cdd90566a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aea45-9362-4f71-b498-19fc18c99147",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71042ed3-da29-43e1-9e08-76e39c172c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    file[\"tmp_jsons\"] = create_segment_transcripts(file, language=LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8c7-3e1a-4991-b008-e10d57c18975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create txt file with all spoken text split by speaker\n",
    "\n",
    "You can edit names of the speakers below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31a9e-9538-4942-8457-43d6f547dc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speakers = {\n",
    "    'SPEAKER_00': ('SPEAKER_00', '#e1ffc7', 'darkgreen'),\n",
    "    'SPEAKER_01': ('SPEAKER_01', 'white', 'darkorange'),\n",
    "    'SPEAKER_02': ('SPEAKER_02', 'lightblue', 'darkblue'),\n",
    "    'SPEAKER_03': ('SPEAKER_03', 'lightpink', 'darkred'),\n",
    "    'SPEAKER_04': ('SPEAKER_04', '#f0e68c', 'goldenrod'),\n",
    "    'SPEAKER_05': ('SPEAKER_05', 'lightcoral', 'darkred'),\n",
    "    'SPEAKER_06': ('SPEAKER_06', '#98fb98', 'forestgreen'),\n",
    "    'SPEAKER_07': ('SPEAKER_07', '#dda0dd', 'purple'),\n",
    "    'SPEAKER_08': ('SPEAKER_08', '#ffcccb', 'darkred'),\n",
    "    'SPEAKER_09': ('SPEAKER_09', 'lightcyan', 'darkcyan')\n",
    "}\n",
    "def_boxclr = 'white'\n",
    "def_spkrclr = 'orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837f50-d1d8-495c-bc18-73f0c8fcb698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcriptFolder = f\"{dataOut}/transcripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62190660-184a-43bd-83cc-e39e7d6b0a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir {transcriptFolder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b816a-f950-4f8d-be11-39d7ab07b1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timeStr(t):\n",
    "    return '{0:02d}:{1:02d}:{2:06.2f}'.format(round(t // 3600), round(t % 3600 // 60), t % 60)\n",
    "def create_capfile(file):\n",
    "    txt = list(\"\")\n",
    "    gidx = -1\n",
    "    last_speaker = \"\"\n",
    "    for g in file[\"groups\"]:  \n",
    "        shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        shift = millisec(shift) - spacermilli #the start time in the original video\n",
    "        shift=max(shift, 0)\n",
    "\n",
    "        gidx += 1\n",
    "        o = f\"{file['tmp_folder']}/\"\n",
    "        captions = json.load(open(f\"{o}{gidx}.json\"))['segments']\n",
    "        \n",
    "        if captions:\n",
    "            speaker = g[0].split()[-1]\n",
    "            boxclr = def_boxclr\n",
    "            spkrclr = def_spkrclr\n",
    "        else:\n",
    "            continue\n",
    "        if speaker in speakers:\n",
    "            speaker, boxclr, spkrclr = speakers[speaker] \n",
    "\n",
    "        tmp_txt = \"\"\n",
    "        tmp_timestamp = \"\"\n",
    "        \n",
    "        for c in captions:\n",
    "            start = shift + c['start'] * 1000.0 \n",
    "            start = start / 1000.0   #time resolution ot youtube is Second.            \n",
    "            end = (shift + c['end'] * 1000.0) / 1000.0   \n",
    "            if speaker != last_speaker:\n",
    "                # print(speaker)\n",
    "                tmp_txt = \"\"\n",
    "                tmp_timestamp = f'[{timeStr(start)} --> {timeStr(end)}] [{speaker}]'\n",
    "            tmp_txt += f'{c[\"text\"]}'\n",
    "\n",
    "            last_speaker = speaker\n",
    "        txt.append(f'{tmp_timestamp}{tmp_txt}\\n')\n",
    "\n",
    "\n",
    "    capfile = f\"{transcriptFolder}/{file['name']}_capspeaker.txt\"\n",
    "\n",
    "    with open(capfile, \"w\", encoding='utf-8') as file:\n",
    "        s = \"\".join(txt)\n",
    "        file.write(s)\n",
    "        print(f'captions saved to {capfile}:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67502645-dc98-47f1-a551-ffca8e44ba75",
   "metadata": {},
   "source": [
    "# Create the actual transcribed files with speaker differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ad7ff-1762-4e18-a6fa-2ec93a76ba85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    create_capfile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d84a84-71e1-451e-b0fd-326eef95a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad844f12-ba8f-4e40-9186-119e96625c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
