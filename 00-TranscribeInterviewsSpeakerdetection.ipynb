{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246b310-5eaa-4b55-b345-44da582da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these if not there\n",
    "# !pip install pydub\n",
    "# !pip install light-the-torch\n",
    "# !pip install openai-whisper\n",
    "# !pip install  git+https://github.com/hmmlearn/hmmlearn.git\n",
    "# !pip install  git+https://github.com/pyannote/pyannote-audio.git@develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543405c4-2568-4783-b64c-5d6ff91915ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import torch\n",
    "from datetime import timedelta\n",
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535a3ad0-e565-4605-9938-4c52d44b333b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0034b98c-7eac-432a-918f-eb80c2042644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac5fe2-835d-4447-b15b-6acfa801b7d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### With first usage of whisper first needs to install/update \"numba.jit\" - happens automatically - 1.42gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798b481-e4f6-4f08-bbdb-a007cad43f86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate a token from hugginface and insert it below\n",
    "1. visit hf.co/pyannote/speaker-diarization and accept user conditions\n",
    "2. visit hf.co/pyannote/segmentation and accept user conditions\n",
    "3. visit hf.co/settings/tokens to create an access token\n",
    "4. instantiate pretrained speaker diarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de37dd2-e202-4028-94b0-0ccc99ba2412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_token = \"<HuggingFace_TOKEN>\" # Add your huggingface token here - see above to see how to get one\n",
    "NUM_SPEAKERS = None # None does autodetection, if you only have 2 speakers try adding a 2 here - I do not yet know how good each option works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27412d07-05a1-41e5-9727-b14f1ea87c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# convert m4a to wav and create subfolder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92800905-4267-4e00-8147-08a64619915d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koschi\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'data/koschi.m4a':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2023-07-28T09:16:07.000000Z\n",
      "  Duration: 00:45:00.22, start: 0.000000, bitrate: 127 kb/s\n",
      "  Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 32000 Hz, mono, fltp, 126 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-07-28T09:16:07.000000Z\n",
      "      handler_name    : AAC audio\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'data/koschi/koschi.wav':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    ISFT            : Lavf58.76.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 32000 Hz, mono, s16, 512 kb/s (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2023-07-28T09:16:07.000000Z\n",
      "      handler_name    : AAC audio\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 pcm_s16le\n",
      "size=  168764kB time=00:45:00.19 bitrate= 512.0kbits/s speed=1.17e+03x    \n",
      "video:0kB audio:168764kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000045%\n"
     ]
    }
   ],
   "source": [
    "data = \"./data\" # Folder with all your m4a audio files right next to this file\n",
    "if not os.path.isdir(data):\n",
    "    !mkdir {data}\n",
    "pathlist = Path(data).glob('**/*.m4a') # Change m4a to your audio file format\n",
    "audio_files = []\n",
    "for audio_file in pathlist:\n",
    "    f = f\"{str(audio_file).split('.')[0]}\"\n",
    "    print(f.split(\"/\")[1])\n",
    "    # Create folders\n",
    "    if not os.path.isdir(f):\n",
    "        !mkdir {f} \n",
    "    ftmp = f\"{f}/tmp\"\n",
    "    if not os.path.isdir(ftmp):\n",
    "        !mkdir {ftmp}\n",
    "    fa = f\"{f}/{str(audio_file).split('/')[1].split('.')[0]}.wav\"\n",
    "    tmp = {\"name\": f.split(\"/\")[1], \"og_audio\": fa, \"tmp_folder\": ftmp}\n",
    "    audio_files.append(tmp)\n",
    "    # !ffmpeg -y -i {audio_file} -ss 00:00:00 -to 00:00:30 {fa} # create files which are only 2min long to test\n",
    "    !ffmpeg -y -i {audio_file} {fa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f52ab961-da20-4f7b-b4dd-c9893e3425b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'koschi',\n",
       "  'og_audio': 'data/koschi/koschi.wav',\n",
       "  'tmp_folder': 'data/koschi/tmp'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a4cf4-6b8a-4f69-b716-c48ec5ae375f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Append time before actual audio, else pyannot might not pick up the first 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50dd1d4-a913-4b77-ae54-8bda4a783ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'koschi',\n",
       "  'og_audio': 'data/koschi/koschi.wav',\n",
       "  'tmp_folder': 'data/koschi/tmp',\n",
       "  'proc_audio': 'data/koschi/koschi_prep.wav'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacermilli = 2000\n",
    "for file in audio_files:\n",
    "    processed_audio_file = file[\"og_audio\"]\n",
    "    processed_audio_file2 = file[\"og_audio\"].split(\".\")[0] +  \"_prep.wav\"\n",
    "    file[\"proc_audio\"] = processed_audio_file2\n",
    "    spacer = AudioSegment.silent(duration=spacermilli)\n",
    "    audio = AudioSegment.from_wav(processed_audio_file) \n",
    "    audio = spacer.append(audio, crossfade=0)\n",
    "    audio.export(processed_audio_file2, format='wav')\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b72ca-1248-4546-a44a-dc64645ee1a7",
   "metadata": {},
   "source": [
    "# Init Pyannot pipeline for speaker identification and whisper for transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bdf3016-1e8d-4898-907c-100059f66c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.0.1+cu117. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1.1\", use_auth_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "840063dd-a6ec-4040-a0b4-5da83202820a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a bit\n",
    "model = whisper.load_model('large') # Can use 'small' or 'base' if used on a local PC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb70460-c79b-4e72-8bcd-e518a7d1ae9b",
   "metadata": {},
   "source": [
    "# Create diarization file (when does a new speaker speak) - this will take a long while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef299360-a083-4a55-a10e-4a7bb6111108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diarization_file(audiofile, numspeaker):\n",
    "    if numspeaker == None:\n",
    "        dz = pipeline(audiofile)  \n",
    "    else:\n",
    "        dz = pipeline(audiofile, num_speakers=numspeaker) \n",
    "    pipeline.to(device) # this should make the pipeline use the GPU, but times are still too slow IMO \n",
    "    diarization_file = audiofile.split(\".\")[0] + \"_dia.txt\"\n",
    "    print(diarization_file)\n",
    "    if os.path.isfile(diarization_file):\n",
    "        return diarization_file\n",
    "    with open(diarization_file, \"w\") as text_file:\n",
    "        text_file.write(str(dz))\n",
    "    return diarization_file\n",
    "\n",
    "def millisec(timeStr):\n",
    "    spl = timeStr.split(\":\")\n",
    "    s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e4d49d-9052-4a9c-b8fe-d9f2890c6a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in audio_files:\n",
    "    file[\"dia_file\"] = create_diarization_file(file[\"proc_audio\"], NUM_SPEAKERS)\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba0b9e-f6de-47af-95a9-e791237f2387",
   "metadata": {},
   "source": [
    "# Create audio segments based on the previous timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833b953-2f77-4c01-84db-b14c31834910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments(file):\n",
    "    dzs = open(file[\"dia_file\"]).read().splitlines()\n",
    "    groups = []\n",
    "    g = []\n",
    "    lastend = 0\n",
    "    for d in dzs:   \n",
    "        if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
    "            groups.append(g)\n",
    "            g = []\n",
    "\n",
    "        g.append(d)\n",
    "\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
    "        end = millisec(end)\n",
    "        if (lastend > end):       #segment engulfed by a previous segment\n",
    "            groups.append(g)\n",
    "            g = [] \n",
    "        else:\n",
    "            lastend = end\n",
    "    if g:\n",
    "        groups.append(g)\n",
    "    audio = AudioSegment.from_wav(file[\"proc_audio\"])\n",
    "    \n",
    "    gidx = -1\n",
    "    tmp_files = []\n",
    "    for g in groups:\n",
    "        start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
    "        start = millisec(start) #- spacermilli\n",
    "        end = millisec(end)  #- spacermilli\n",
    "        gidx += 1\n",
    "        fname = f\"{file['tmp_folder']}/{gidx}.wav\"\n",
    "        tmp_files.append(fname)\n",
    "        audio[start:end].export(fname, format='wav')\n",
    "    print(tmp_files[:10])\n",
    "    return tmp_files, groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3505d-de58-4b2c-beea-a1ba5325f1bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for file in audio_files:\n",
    "    file[\"tmp_wavs\"], file[\"groups\"] = create_segments(file)\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b299c-b201-47bb-87dd-4b97d622556f",
   "metadata": {},
   "source": [
    "# Create Transcripts for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe677c9-4616-4e3b-ad4c-ecf085b95760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_segment_transcripts(file):\n",
    "    # print(len(file[\"tmp_wavs\"]))\n",
    "    for i in range(len(file[\"tmp_wavs\"])):\n",
    "        audiof = str(i) + '.wav'\n",
    "        segment_audio = f\"{file['tmp_folder']}/{audiof}\"\n",
    "        result = model.transcribe(audio=segment_audio, language='en', word_timestamps=True)#, initial_prompt=result.get('text', \"\"))\n",
    "        o = f\"{file['tmp_folder']}/\"\n",
    "        tmp_files = []\n",
    "        # print(f\"{o}{i}.json\")\n",
    "        with open(f\"{o}{i}.json\", \"w\") as outfile:\n",
    "            tmp_files.append(str(outfile))\n",
    "            json.dump(result, outfile, indent=4) \n",
    "    return tmp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71042ed3-da29-43e1-9e08-76e39c172c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for file in audio_files:\n",
    "    file[\"tmp_jsons\"] = create_segment_transcripts(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07b8c7-3e1a-4991-b008-e10d57c18975",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creates txt file with all spoken text split by speaker\n",
    "\n",
    "You can edit names of the speakers below, colors are currently not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d31a9e-9538-4942-8457-43d6f547dc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speakers = {\n",
    "    'SPEAKER_00': ('SPEAKER_00', '#e1ffc7', 'darkgreen'),\n",
    "    'SPEAKER_01': ('SPEAKER_01', 'white', 'darkorange'),\n",
    "    'SPEAKER_02': ('SPEAKER_02', 'lightblue', 'darkblue'),\n",
    "    'SPEAKER_03': ('SPEAKER_03', 'lightpink', 'darkred'),\n",
    "    'SPEAKER_04': ('SPEAKER_04', '#f0e68c', 'goldenrod'),\n",
    "    'SPEAKER_05': ('SPEAKER_05', 'lightcoral', 'darkred'),\n",
    "    'SPEAKER_06': ('SPEAKER_06', '#98fb98', 'forestgreen'),\n",
    "    'SPEAKER_07': ('SPEAKER_07', '#dda0dd', 'purple'),\n",
    "    'SPEAKER_08': ('SPEAKER_08', '#ffcccb', 'darkred'),\n",
    "    'SPEAKER_09': ('SPEAKER_09', 'lightcyan', 'darkcyan')\n",
    "}\n",
    "def_boxclr = 'white'\n",
    "def_spkrclr = 'orange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4003871e-d908-4b02-983c-d8be8ab5ee14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timeStr(t):\n",
    "    return '{0:02d}:{1:02d}:{2:06.2f}'.format(round(t // 3600), round(t % 3600 // 60), t % 60)\n",
    "def create_capfile(file):\n",
    "    txt = list(\"\")\n",
    "    gidx = -1\n",
    "    for g in file[\"groups\"]:  \n",
    "        shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
    "        shift = millisec(shift) - spacermilli #the start time in the original video\n",
    "        shift=max(shift, 0)\n",
    "\n",
    "        gidx += 1\n",
    "        o = f\"{file['tmp_folder']}/\"\n",
    "        captions = json.load(open(f\"{o}{gidx}.json\"))['segments']\n",
    "\n",
    "        if captions:\n",
    "            speaker = g[0].split()[-1]\n",
    "            boxclr = def_boxclr\n",
    "            spkrclr = def_spkrclr\n",
    "        if speaker in speakers:\n",
    "            speaker, boxclr, spkrclr = speakers[speaker] \n",
    "\n",
    "\n",
    "        for c in captions:\n",
    "            start = shift + c['start'] * 1000.0 \n",
    "            start = start / 1000.0   #time resolution ot youtube is Second.            \n",
    "            end = (shift + c['end'] * 1000.0) / 1000.0      \n",
    "            txt.append(f'[{timeStr(start)} --> {timeStr(end)}] [{speaker}] {c[\"text\"]}\\n')\n",
    "\n",
    "\n",
    "    capfile = f\"{file['proc_audio'].split('.')[0]}_capspeaker.txt\"\n",
    "\n",
    "    with open(capfile, \"w\", encoding='utf-8') as file:\n",
    "        s = \"\".join(txt)\n",
    "        file.write(s)\n",
    "        print(f'captions saved to {capfile}:')\n",
    "        # print(s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67502645-dc98-47f1-a551-ffca8e44ba75",
   "metadata": {},
   "source": [
    "# Create the actual transcribed files with speaker differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ad7ff-1762-4e18-a6fa-2ec93a76ba85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in audio_files:\n",
    "    create_capfile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5829c8b0-eec4-4500-821e-a11d9b31e490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 36 µs, sys: 1e+03 ns, total: 37 µs\n",
      "Wall time: 48.9 µs\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_testing",
   "language": "python",
   "name": "whisper_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
